## hyperparameter_searches

This folder contains two subdirectories that together contain the full results from most of my scikit-learn hyperparameter grid and randomized searches. In total, I ran 14 hyperparameter searches to find the optimal hyperparameters for my models; these 14 are summarized (with the classification accuracy of the optimal hyperparameter combination for each) in the slide [here](https://docs.google.com/presentation/d/1-4lD1JnZ_y5GpXJUkj3m67tK97MZ3NX92SAqbPB1eN4/edit?usp=sharing). The three runs in the "1st W&B (no CV)" row were done using the Weights & Biases (W&B) _sweep_ functionality for searching over hyperparameters, which does not involve cross-validation. Information on the different models trained with different sets of hyperparameters for these three sweeps is recorded in W&B [here](https://wandb.ai/parsellsx/tess/sweeps) under the names test-sweep-3 (this is the original support vector classifier (SVC) sweep), forest-sweep-1 (first random forest (RF) sweep), and kneigh-sweep-1 (first k-nearest-neighbors (KNN) sweep). The other sweeps were tests I ran earlier that aren't relevant. 

The runs in the second row of the slide ("Sklearn") were done with the GridSearchCV() function of scikit-learn, which includes cross-validation. The RF and KNN runs in this row are recorded at 'hyperparameter_searches/sector_14_gridsearchcv_dfs/rf_gridcv_1.zip' and 'hyperparameter_searches/sector_14_gridsearchcv_dfs/kneigh_gridcv_1.zip'. 

The runs in the third row of the slide ("Sklearn (no long-period EBs)") were the first runs I did with the long-period EBs (P > 15 days) removed from the Kepler data. They were also done with the [GridSearchCV()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) function of scikit-learn. All three runs' results are recorded in the 'hyperparameter_searches/sector_14_gridsearchcv_dfs' folder at 'svc_gridcv_short_period_1.zip', 'rf_gridcv_short_period_1.zip', and 'kneigh_gridcv_short_period_1.zip'. 

The runs in the fourth row of the slide ("Sklearn (fine grained)") were the first runs I did on the aparsells-sweep virtual machine. I initially planned to do a very fine-grained grid search with GridSearchCV() for all three of these runs, but I found that the parameter space I wanted to explore for the RF and KNN runs was too large for a grid search to be practical. Instead, I did a grid search for the SVC run and a randomized search (with the scikit-learn [RandomizedSearchCV()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV) function) for the RF and KNN runs. The SVC run is recorded at 'hyperparameter_searches/sector_14_gridsearchcv_dfs/svc_gridcv_short_period_2.zip', and the other two are recorded under 'hyperparameter_searches/sector_14_randomsearchcv_dfs' at 'rf_randomcv_short_period_1.zip' and 'kneigh_randomcv_short_period_1.zip'. 

The runs in the fifth row of the slide ("Sklearn (fine; new train/test split)") were done with only the SVC and RF classifiers (same hyperparameters as the fourth row). I used a new split of the dataset between training and test data to see if I would achieve similar accuracy to the runs in the fourth row of the slide. Specifically, I wanted to make sure that the accuracy I was achieving wasn't due to overfitting on the test set. I achieved similar accuracy (see the slide), so I feel confident that I wasn't overfitting and that the accuracy I achieved is reproducible with different datasets. These two runs are accessible at 'hyperparameter_searches/sector_14_gridsearchcv_dfs/svc_gridcv_short_period_3.zip' and 'hyperparameter_searches/sector_14_randomsearchcv_dfs/rf_randomcv_short_period_2.zip'. 

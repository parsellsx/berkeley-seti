# berkeley-seti
This repository contains Python code and related data files for making a machine learning classifier to categorize variable star light curves from the TESS satellite. I (Alex Parsells) worked on this project with guidance from Daniel Giles (SETI Institute), Ann Marie Cody (SETI Institute), and Steve Croft (Berkeley SETI Research Center) over 10 weeks in summer 2021 as part of an internship with Berkeley SETI Research Center. This work was supported by the National Science Foundation through its Research Experiences for Undergraduates (REU) program.

## Project Background and Related Work
The goal of this project is to develop a machine learning (ML) model that can automatically classify TESS light curves into their respective variable star categories. Such a classifier can then be used in conjunction with an anomaly detection pipeline (see Giles & Walkowicz, 2020) which is designed to take in thousands of light curves (more than could reasonably be vetted by eye) and flag any whose brightness patterns seem "weird" or unusual. Those unusual light curves can then be passed into a well-trained classifier to see if it identifies them as members of a particular variable star class, or if it can't determine a likely class. In the latter case, the stars in question might be well-suited for follow-up observation to determine if they are in fact members of a rare variable type, a totally new variable type, or even candidates to have transiting megastructures (e.g., Dyson spheres) built by extraterrestrial intelligence in orbit around them. Searching for stars in this last category is where the SETI relevance of this project lies.

## My Work in Summer 2021
I focused on a reduced classification problem for this summer with only two classes available for each light curve: eclipsing binary (EB) or non-eclipsing-binary. The first thing I did was look for data that could be used to train a model. Our group is in possession of the largest collection of TESS light curves currently available (stored in the Breakthrough Listen Google Cloud Platform storage bucket called tess-goddard-lcs), so finding light curves was not a problem. The difficulty was identifying catalogs of stars that had been confirmed to be eclipsing binaries, and then finding tools to get the TESS Input Catalog (TIC) IDs for these stars. I initially queried SIMBAD for eclipsing binaries ('catalogs/simbad_eclipsing_binaries_20000_stars.txt'), but getting those stars' TIC IDs from their SIMBAD identifiers proved difficult (see 'Eclipsing Binaries Get TIC IDs SIMBAD.ipynb') since the astroquery Python package only supports querying SIMBAD for alternate object names for a single object at a time, and SIMBAD will temporarily lock out any user who submits too many queries in a short time. Because of these issues, plus the fact that I desired more thoroughly vetted EB classifications than SIMBAD can provide, I decided to look elsewhere for EBs. I searched [ADS](https://ui.adsabs.harvard.edu/) for catalogs of eclipsing binaries found by TESS, and I later expanded my search to any catalogs of eclipsing binaries. Several of these catalogs are in the __catalogs__ folder and, with a little work to get the TIC IDs, can be used to train future models. 

Ultimately, I decided to use the [Villanova Kepler EBs dataset](keplerebs.villanova.edu), which consists of 2922 EBs, listed by Kepler Input Catalog (KIC) ID. I initially filtered these by their listed magnitudes (which are Kepler magnitudes), keeping only those between 10 and 15 ('catalogs/keplerebs_villanova_kmag_10-15.csv'). However, after playing with the data a bit (see 'notebooks_messy/Kepler EBs to TIC IDs and Filepaths.ipynb' and the resulting 'input_data/cleaned_sector_14_ebs_filepaths.csv'), I realized (in the middle of 'notebooks_messy/Sector 14 Kepler EBs Lightcurve Loading and Featurization.ipynb') that there might be some stars with K-mags outside of the 10-15 range but with T-mags (TESS mags) within that range, and I would want to keep those stars. As a result, I restarted with the entire Kepler EBs dataset ('catalogs/keplerebs_villanova_complete.csv'). 

In order to get the TIC IDs that matched up with my list of EB KIC IDs, I used Jim Davenport's Github repo [KIC2TIC](https://github.com/jradavenport/kic2tic), which contains a large list of stars by both KIC ID and TIC ID. Using that file, I got TIC IDs for each of my 2172 EBs and then set about getting a similar number of non-EBs to round out the data I would use to train and test my first classifier. I took a random sample of 2000 light curves from stars that were 1) present in our TESS sector 14 light curves folder (GCP storage directory 'tesslcs/sector14lookup.csv'), 2) not present in the Villanova Kepler EBs dataset, and 3) not present in 'catalogs/ebcat_tics.txt', a file compiled by Ethan Kruse and Brian Powell that contains TIC IDs of probable EBs. Note, however, that I didn't use the TIC IDs in this file as a source of ground truth for EBs because it has a non-negligible number of false positives included and false negatives excluded.

Once I got a list of non-EBs, I combined it with the list of EBs to produce 'input_data/sector_14_ebs_nonebs_final_input_df.csv' (df stands for pandas DataFrame). I then used code from Daniel Giles' Github repo [SPOcc](https://github.com/d-giles/SPOcc/tree/main/spocc) to load in the light curves listed in the final_input_df and featurize them with 29 features I selected from the [Cesium Python library](https://cesium-ml.org/). I saved the feature set in 'input_data/sector_14_ebs_nonebs_features.csv'. With the labels (EB/Non-EB) from final_input_df and with the feature set, I was able to train my first classifier, a linear support vector classifier from scikit-learn (scikit-learn is the ML library I used for all of my models). It achieved a classification accuracy of 0.7637.

After training my first model, I began experimenting with other models in 'Experimenting with Classifiers on Kepler Sector 14 EB Data.ipynb'. In that notebook I did my first search over hyperparameters with scikit-learn's GridSearchCV(). I also set up [Weights & Biases](https://wandb.ai/), a web interface that I used to keep track of all of my models, and ran a set of three "sweeps" (hyperparameter grid searches without cross-validation) on there with a support vector classifier (SVC), a random forest classifier (RF), and a k-nearest neighbors classifier (KNN). These sweeps improved my best classification accuracy to around 0.8. Upon realizing that W&B sweeps don't involve cross-validation (CV), I redid the same searches with CV using scikit-learn and got similar (slightly lower) accuracies. 

I next removed all EBs with periods longer than 15 days from my data, since a single TESS sector is only 27 days of observation, and longer-period EBs would have less than 2 periods visible in their sector 14 light curves. (See 'input_data/sector_14_final_input_df_short_period_8-2-21.zip' and 'input_data/sector_14_fset_short_period_8-2-21.zip'.) Retraining the three models with GridSearchCV(), I found that removing the long-period EBs appreciably reduced the number of false positive identifications and improved my maximum accuracy to about 0.83. 

After noticing this performance improvement, I decided to start a second virtual machine (aparsells-sweep; my first one was aparsells-test2) on GCP and run finer-grained hyperparameter searches on it in order to explore more of hyperparameter space and hopefully improve performance further (see 'Fine-Tuning Hyperparameters on Sector 14 EB Data.ipynb'). I used a mix of the scikit-learn functions GridSearchCV() and RandomizedSearchCV() to search more of hyperparameter space, and I achieved an increased peak accuracy of 0.8476 with an RF classifier (currently my highest accuracy). I also succeeded in nearly duplicating this performance with a new train/test split in my data, demonstrating that my models were not achieving this accuracy by overfitting to the test set but through generalizable methods. 

### Next Steps for this Project
That's all the work I accomplished during the summer internship (from June to mid-August, 2021), but there are several exciting next steps to explore. One is the use of an autoencoder to automate feature extraction from the light curves. Currently, all of my models have been trained on a set of 29 features that I hand-selected from the Cesium library based on what I thought (with little experience) would be useful in differentiating EBs from non-EBs. Autoencoders, on the other hand, can be trained to pick out the most relevant features automatically, and these features might be things that no human would think of. 

## How to Use this Repository
Code is stored in Jupyter Notebooks in one of two directories: notebooks_streamlined and notebooks_messy. The notebooks_streamlined folder contains the core code required to reproduce my results and can be easily modified to add new experiments. The notebooks_messy folder contains most of the same notebooks, but as I originally wrote them, with few or no modifications. As such, these notebooks contain all of my false starts/dead ends in trying to achieve a particular goal, as well as many notes I took while acquiring data, training models, etc. I include them so that if something in the streamlined notebooks is unclear or someone wants to know exactly the thought process I went through in deciding to do something a certain way, they can find that out.

Each directory in the repository has its own README.md file which describes in more detail the files in that directory. Here are the general contents of each directory:
* __catalogs__ contains raw (unmodified) data files downloaded from astronomical research papers that catalog eclipsing binaries. These files include keplerebs_villanova_complete.csv, which with some modifications (see the __input_data__ directory) is the primary set of EBs I used to train models
* __hyperparameter_searches__ contains two sub-folders which together contain information on the configuration (hyperparameters) and performance of every model in almost all of my scikit-learn grid and randomized searches over hyperparameters. Specifically, they contain pandas DataFrame versions of the sklearn.model_selection.GridSearchCV() and RandomizedSearchCV() attributes 'cv_results_'. The README in this folder contains details on each of the 14 grid and randomized hyperparameter searches that I ran for this project
* __input_data__ contains modified versions of the data files in __catalogs__. This includes filepaths for each eclipsing binary in the Google Cloud Platform storage bucket, as well as feature sets for the light curves (made with the Cesium Python library) and a pre-made set of train/test data that can be loaded into a model. Using this train/test split can help make results reproducible by avoiding calling the scikit-learn function train_test_split() multiple times and getting different results (though note that the random_state parameter of that function can also achieve this purpose)
* __notebooks_messy__ contains original versions of all the code I wrote along the way, including attempts to get EB TIC IDs from SIMBAD with astroquery, reading in the Kepler EBs dataset and getting TIC IDs for each EB, and training ML models
* __notebooks_streamlined__ contains the cleaned-up notebooks from __notebooks_messy__. I've gotten rid of a lot of unnecessary notes and dead ends in my coding process to include only what you actually need to read in an EB dataset and ultimately train a variety of ML models with it
* __pseti_poster__ contains the poster I presented for this project at the Penn State PSETI Center's 2021 Assembly of the Order of the Octopus conference
* __sklearn_plots_for_wandb__ contains confusion matrices and precision-recall plots for the single best models from each of my scikit-learn hyperparameter searches. I store them in this directory for future reference and because they need to be stored somewhere to be uploaded and logged to Weights & Biases
* __wandb__ contains internal log files from Weights & Biases

## References
1. Giles & Walkowicz, 2020
2. Cesium

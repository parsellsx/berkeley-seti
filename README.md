# berkeley-seti
This repository contains Python code and related data files for making a machine learning classifier to categorize variable star light curves from the TESS satellite. I (Alex Parsells) worked on this project with guidance from Daniel Giles (SETI Institute), Ann Marie Cody (SETI Institute), and Steve Croft (UC Berkeley) over 10 weeks in summer 2021 as part of an internship with Berkeley SETI Research Center. This work was supported by the National Science Foundation through its Research Experiences for Undergraduates (REU) program.

## Project Background and Related Work
The goal of this project is to develop a machine learning (ML) model that can automatically classify TESS light curves into their respective variable star categories. Such a classifier can then be used in conjunction with an anomaly detection pipeline (see Giles & Walkowicz, 2020) which is designed to take in thousands of light curves (more than could reasonably be vetted by eye) and flag any whose brightness patterns seem "weird" or unusual. Those unusual light curves can then be passed into a well-trained classifier to see if it identifies them as members of a particular variable star class, or if it can't determine a likely class. In the latter case, the stars in question might be well-suited for follow-up observation to determine if they are in fact members of a rare variable type, a totally new variable type, or even candidates to have transiting megastructures (e.g., Dyson spheres) built by extraterrestrial intelligence in orbit around them. Searching for stars in this last category is where the SETI relevance of this project lies.

## My Work in Summer 2021
I focused on a reduced classification problem for this summer with only two classes available for each light curve: eclipsing binary (EB) or non-eclipsing-binary. The first thing I did was look for data that could be used to train a model. Our group is in possession of the largest collection of TESS light curves currently available (stored in the Breakthrough Listen Google Cloud Platform storage bucket called tess-goddard-lcs), so finding light curves was not a problem. The difficulty was identifying catalogs of stars that had been confirmed to be eclipsing binaries, and then finding tools to get the TESS Input Catalog (TIC) IDs for these stars. I initially queried SIMBAD for eclipsing binaries ('catalogs/simbad_eclipsing_binaries_20000_stars.txt'), but getting those stars' TIC IDs from their SIMBAD identifiers proved difficult

## How to Use this Repository
Code is stored in Jupyter Notebooks in one of two directories: notebooks_streamlined and notebooks_messy. The notebooks_streamlined folder contains the core code required to reproduce my results and can be easily modified to add new experiments. The notebooks_messy folder contains most of the same notebooks, but as I originally wrote them, with few or no modifications. As such, these notebooks contain all of my false starts/dead ends in trying to achieve a particular goal, as well as many notes I took while acquiring data, training models, etc. I include them so that if something in the streamlined notebooks is unclear or someone wants to know exactly the thought process I went through in deciding to do something a certain way, they can find that out.

Each directory in the repository has its own README.md file which describes in more detail the files in that directory. Here are the general contents of each directory:
* __catalogs__ contains raw (unmodified) data files downloaded from astronomical research papers that catalog eclipsing binaries. These files include keplerebs_villanova_complete.csv, which with some modifications (see the __input_data__ directory) is the primary set of EBs I used to train models
* __input_data__ contains modified versions of the data files in __catalogs__. This includes filepaths for each eclipsing binary in the Google Cloud Platform storage bucket, as well as feature sets for the light curves (made with the Cesium Python library) and a pre-made set of train/test data that can be loaded into a model. Using this train/test split can help make results reproducible by avoiding calling the scikit-learn function train_test_split() multiple times and getting different results (though note that the random_state parameter of that function can also achieve this purpose)
* __notebooks_messy__ contains original versions of all the code I wrote along the way, including attempts to get EB TIC IDs from SIMBAD with astroquery, reading in the Kepler EBs dataset and getting TIC IDs for each EB, and training ML models
* __notebooks_streamlined__ contains the cleaned-up notebooks from __notebooks_messy__. I've gotten rid of a lot of unnecessary notes and dead ends in my coding process to include only what you actually need to read in an EB dataset and ultimately train a variety of ML models with it
* __pseti_poster__ contains the poster I presented for this project at the Penn State PSETI Center's 2021 Assembly of the Order of the Octopus conference
* __sklearn_plots_for_wandb__ contains confusion matrices and precision-recall plots for the single best models from each of my scikit-learn hyperparameter searches. I store them in this directory for future reference and because they need to be stored somewhere to be uploaded and logged to Weights & Biases
* __wandb__ contains internal log files from Weights & Biases

## References
1. Giles & Walkowicz, 2020
2. Cesium
